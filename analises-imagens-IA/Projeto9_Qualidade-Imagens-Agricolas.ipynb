{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8feaeef3",
   "metadata": {},
   "source": [
    "# Projeto Análise de Qualidade de Alimentos em Plantações Agrícola com IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913886f",
   "metadata": {},
   "source": [
    "O objetivo é fazer o fine-tuningem um modelo Vision Transformer Pré-treinado  e  ajustá-lo  ao  nosso  próprio  caso  de  uso, a  fim  de  classificar  e  prever  a  qualidade  de alimentos  em  plantações  agrícolas. \n",
    "\n",
    "Com base em uma imagem de folha, o  objetivo desta tarefa é prever o tipo de doença (Mancha Angular e Ferrugem do Feijão), se houver.Os termos em inglês são Angular Leaf Spot eBean Rust.\n",
    "\n",
    "Fonte dos dados: https://huggingface.co/datasets/beans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff84603",
   "metadata": {},
   "source": [
    "## 1. Instalando e carregando os pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4cbb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.13\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51966159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "# Ocultas avisos do Tensorflow\n",
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f97b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala Torch\n",
    "!pip install -q torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea033f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala Transformers\n",
    "!pip install -q transformers==4.30.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1899a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala datasets\n",
    "!pip install -q datasets==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893c3899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\morai\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\morai\\anaconda3\\lib\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\morai\\anaconda3\\lib\\site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\morai\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\morai\\anaconda3\\lib\\site-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\morai\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\morai\\anaconda3\\lib\\site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: networkx in c:\\users\\morai\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\morai\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\morai\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\morai\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.10.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\morai\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\morai\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\morai\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "# Instala accelerate\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ee5fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import accelerate\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c2c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Projeto Análise Imagens Agrícolas com IA\n",
      "\n",
      "torch       : 2.0.1\n",
      "datasets    : 2.11.0\n",
      "requests    : 2.28.1\n",
      "numpy       : 1.21.5\n",
      "PIL         : 9.2.0\n",
      "transformers: 4.30.2\n",
      "accelerate  : 0.21.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Projeto Análise Imagens Agrícolas com IA\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ec253",
   "metadata": {},
   "source": [
    "## 2. Aplicando o Fine-Tunning ao Modelo\n",
    "Pré-processamento para processar os dados do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db16bf",
   "metadata": {},
   "source": [
    "### 2.1 Carga do dataset no disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce1e3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset beans (C:/Users/morai/.cache/huggingface/datasets/beans/default/0.0.0/90c755fb6db1c0ccdad02e897a37969dbf070bed3755d4391e269ff70642d791)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46baf2444b374b30ad0cece1b25db82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carrega os dados\n",
    "dados = load_dataset('beans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a977bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image_file_path', 'image', 'labels'],\n",
      "        num_rows: 1034\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image_file_path', 'image', 'labels'],\n",
      "        num_rows: 133\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image_file_path', 'image', 'labels'],\n",
      "        num_rows: 128\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Visualiza os dados\n",
    "print(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1ce35e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'], id=None)\n"
     ]
    }
   ],
   "source": [
    "# Extrai os labels\n",
    "labels = dados['train'].features['labels']\n",
    "\n",
    "# Visualiza os dados\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0dc0a",
   "metadata": {},
   "source": [
    "### 2.2 Aplicando o ViT Feature Extractor para Processar as Imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2cf40",
   "metadata": {},
   "source": [
    "O ViT Feature Extractor serve para transformar imagens de entrada em representações vetoriais de alto nível que podem ser usadas para uma variedade de tarefas além da classificação de imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "580730ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repositório do ViT pré-treinado\n",
    "repo_id = 'google/vit-base-patch16-224-in21k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99798639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o ViTFeatureExtractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b228197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualiza o FeatureExtractor\n",
    "print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "017c5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para o mapeamento de lotes de imagens e alpicação do ViTFeatureExtractor\n",
    "def transform(example_batch):\n",
    "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors = 'pt')\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "584511af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara dos dados\n",
    "prepared_data = dados.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3933aa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.5686, -0.5686, -0.5608,  ..., -0.0275,  0.1922, -0.2549],\n",
       "          [-0.6078, -0.6000, -0.5843,  ..., -0.0353, -0.0196, -0.2706],\n",
       "          [-0.6314, -0.6314, -0.6157,  ..., -0.2392, -0.3647, -0.2314],\n",
       "          ...,\n",
       "          [-0.5373, -0.5529, -0.5765,  ..., -0.0745, -0.0431, -0.0980],\n",
       "          [-0.5608, -0.5765, -0.5843,  ...,  0.3176,  0.1608,  0.1294],\n",
       "          [-0.5843, -0.5922, -0.6078,  ...,  0.2784,  0.1451,  0.2000]],\n",
       "\n",
       "         [[-0.7098, -0.7098, -0.7490,  ..., -0.3725, -0.1608, -0.6000],\n",
       "          [-0.7333, -0.7333, -0.7569,  ..., -0.3569, -0.3176, -0.5608],\n",
       "          [-0.7490, -0.7490, -0.7647,  ..., -0.5373, -0.6627, -0.5373],\n",
       "          ...,\n",
       "          [-0.7725, -0.7882, -0.8196,  ..., -0.2314, -0.0353,  0.0824],\n",
       "          [-0.7961, -0.8118, -0.8118,  ...,  0.1843,  0.3176,  0.3725],\n",
       "          [-0.8196, -0.8196, -0.8275,  ...,  0.0745,  0.2863,  0.3961]],\n",
       "\n",
       "         [[-0.9922, -0.9922, -1.0000,  ..., -0.5451, -0.3647, -0.7333],\n",
       "          [-0.9922, -0.9922, -1.0000,  ..., -0.5686, -0.5451, -0.7176],\n",
       "          [-0.9843, -0.9922, -1.0000,  ..., -0.6549, -0.7490, -0.6314],\n",
       "          ...,\n",
       "          [-0.8431, -0.8588, -0.8980,  ..., -0.5765, -0.5608, -0.5529],\n",
       "          [-0.8588, -0.8902, -0.9137,  ..., -0.2078, -0.2549, -0.2706],\n",
       "          [-0.8824, -0.9059, -0.9294,  ..., -0.2627, -0.1922, -0.1216]]],\n",
       "\n",
       "\n",
       "        [[[-0.5137, -0.4824, -0.4118,  ..., -0.0353, -0.0039, -0.2078],\n",
       "          [-0.4902, -0.4588, -0.4196,  ..., -0.0745, -0.0039, -0.2314],\n",
       "          [-0.4667, -0.4902, -0.5137,  ..., -0.0824, -0.0039, -0.2941],\n",
       "          ...,\n",
       "          [-0.4980, -0.4980, -0.5294,  ..., -0.2000, -0.2157, -0.3882],\n",
       "          [-0.5529, -0.5294, -0.5137,  ..., -0.1922, -0.1922, -0.3882],\n",
       "          [-0.5294, -0.5451, -0.5451,  ..., -0.1294, -0.1529, -0.2706]],\n",
       "\n",
       "         [[-0.1843, -0.2000, -0.1608,  ...,  0.2157,  0.2000, -0.0902],\n",
       "          [-0.1608, -0.1686, -0.1529,  ...,  0.1922,  0.2235, -0.0902],\n",
       "          [-0.1529, -0.2000, -0.2392,  ...,  0.1765,  0.2549, -0.1294],\n",
       "          ...,\n",
       "          [-0.7725, -0.7569, -0.7569,  ..., -0.4196, -0.4588, -0.6471],\n",
       "          [-0.8039, -0.7804, -0.7647,  ..., -0.4196, -0.4510, -0.6627],\n",
       "          [-0.7647, -0.7961, -0.8039,  ..., -0.3725, -0.4196, -0.5451]],\n",
       "\n",
       "         [[-0.7412, -0.8588, -0.8510,  ..., -0.3255, -0.2627, -0.5608],\n",
       "          [-0.7725, -0.8431, -0.8196,  ..., -0.3490, -0.2706, -0.5765],\n",
       "          [-0.8196, -0.8667, -0.8510,  ..., -0.3725, -0.2314, -0.5451],\n",
       "          ...,\n",
       "          [-0.5216, -0.5137, -0.5294,  ..., -0.2471, -0.2549, -0.4510],\n",
       "          [-0.5529, -0.5373, -0.5216,  ..., -0.2157, -0.2157, -0.4431],\n",
       "          [-0.5294, -0.5529, -0.5608,  ..., -0.1608, -0.1843, -0.3333]]]]), 'labels': [0, 0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza um exemplo para conferir se a função está correta\n",
    "prepared_data['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2081aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para combinar as amostras\n",
    "# A função combina múltiplas amostras em um único lote para o processamento do Pytorch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    return{'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "          'labels': torch.tensor([x['labels'] for x in batch])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7267f",
   "metadata": {},
   "source": [
    "## 3. Construção do Módulo de Treino do ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2015169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica do modelo\n",
    "metric = load_metric('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04749391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo da métrica\n",
    "def compute_metrics(prediction):\n",
    "    return metric.compute(predictions = np.argmax(prediction.predictions, axis = 1),\n",
    "                         references = prediction.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "189f14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copia os labels\n",
    "labels = dados['train'].features['labels'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4d4e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angular_leaf_spot', 'bean_rust', 'healthy']\n"
     ]
    }
   ],
   "source": [
    "# Visualiza os labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "175a3b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Importa o modelo ViTForImageClassification indicandos os novos labels que serão usados\n",
    "modelo = ViTForImageClassification.from_pretrained(repo_id,\n",
    "                                                  num_labels = len(labels),\n",
    "                                                  id2label = {str(i):c for i, c in enumerate(labels)},\n",
    "                                                  label2id = {c:str(i) for i, c in enumerate(labels)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8b7ef",
   "metadata": {},
   "source": [
    "Esta mensagem acima está dizendo que você está inicializando um modelo ViTForImageClassification (um modelo de classificação de imagem Vision Transformer) a partir de um modelo pré-treinado (google/vit-base-patch16-224-in21k). No entanto, nem todos os pesos do modelo pré-treinado estão sendo usados na inicialização e alguns pesos do modelo ViTForImageClassification estão sendo inicializados do zero.\n",
    "\n",
    "Há dois pontos principais aqui:\n",
    "\n",
    "Alguns pesos do checkpoint do modelo ('pooler.dense.bias', 'pooler.dense.weight') não foram utilizados na inicialização do ViTForImageClassification. Isso pode ser esperado se você estiver inicializando o ViTForImageClassification a partir do checkpoint de um modelo treinado em outra tarefa ou com outra arquitetura. Se o modelo pré-treinado fosse exatamente idêntico à arquitetura que você está inicializando, você esperaria que todos os pesos fossem usados, e a mensagem informaria que algo está errado se isso não acontecesse.\n",
    "\n",
    "Alguns pesos do ViTForImageClassification ('classifier.bias', 'classifier.weight') não foram inicializados a partir do checkpoint do modelo e foram recém-inicializados. Isso significa que esses componentes específicos do modelo não receberam pesos do modelo pré-treinado e, em vez disso, foram inicializados, provavelmente com alguma forma de inicialização aleatória.\n",
    "\n",
    "A mensagem termina sugerindo que você provavelmente deve treinar (ou seja, fazer um \"fine-tuning\") este modelo em uma tarefa antes de usá-lo para predições e inferências. Isso ocorre porque, embora o modelo tenha sido parcialmente inicializado com pesos de um modelo pré-treinado, ele ainda tem alguns pesos que foram inicializados aleatoriamente e, portanto, precisam ser ajustados para a tarefa específica que você deseja resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3111a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argumentos de treino\n",
    "training_args = TrainingArguments(output_dir = \"resultados\",\n",
    "                                  evaluation_strategy = 'steps',\n",
    "                                  num_train_epochs = 4,\n",
    "                                  learning_rate = 2e-4,\n",
    "                                  remove_unused_columns = False,\n",
    "                                  load_best_model_at_end = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bf9cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(model = modelo,\n",
    "                  args = training_args,\n",
    "                  data_collator = collate_fn,\n",
    "                  compute_metrics = compute_metrics,\n",
    "                  train_dataset = prepared_data['train'],\n",
    "                  eval_dataset = prepared_data['validation'],\n",
    "                  tokenizer = feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f777e7",
   "metadata": {},
   "source": [
    "## 4. Treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9db09c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='520' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [520/520 57:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.052046</td>\n",
       "      <td>0.977444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "216da486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo em disco\n",
    "trainer.save_model('modelos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c0997d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         4.0\n",
      "  total_flos               = 298497957GF\n",
      "  train_loss               =       0.143\n",
      "  train_runtime            =  0:57:29.79\n",
      "  train_samples_per_second =       1.199\n",
      "  train_steps_per_second   =       0.151\n"
     ]
    }
   ],
   "source": [
    "# Log das métricas\n",
    "trainer.log_metrics('train', train_results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08fa495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva as métricas\n",
    "trainer.save_metrics('train', train_results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3af0b5",
   "metadata": {},
   "source": [
    "## 5. Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39e1d1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.9774\n",
      "  eval_loss               =      0.052\n",
      "  eval_runtime            = 0:00:37.65\n",
      "  eval_samples_per_second =      3.532\n",
      "  eval_steps_per_second   =      0.451\n"
     ]
    }
   ],
   "source": [
    "# Avaliação do modelo \n",
    "metrics = trainer.evaluate(prepared_data['validation'])\n",
    "trainer.log_metrics('eval', metrics)\n",
    "trainer.save_metrics('eval', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264556b",
   "metadata": {},
   "source": [
    "## 6. Deploy do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bed0904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de uma imagem (experimente outras imagens)\n",
    "url = 'https://www.greenlife.co.ke/wp-content/uploads/2022/04/disease_bean_angular_leaf_spot.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7282e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega a imagem\n",
    "image = Image.open(requests.get(url, stream = True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2c4522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica o extrator\n",
    "inputs = feature_extractor(images = image, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da958991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloca a imagem no mesmo device do modelo\n",
    "inputs = {name: tensor.to(trainer.args.device) for name, tensor in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83b050ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.6157,  0.6078,  0.6314,  ..., -0.0118, -0.0275, -0.0353],\n",
       "           [ 0.6000,  0.6000,  0.6078,  ..., -0.0118, -0.0431, -0.0667],\n",
       "           [ 0.6000,  0.6157,  0.5922,  ..., -0.0039, -0.0196, -0.0824],\n",
       "           ...,\n",
       "           [ 0.4353,  0.4039,  0.4118,  ...,  0.0275,  0.0039,  0.0118],\n",
       "           [ 0.4275,  0.4196,  0.4353,  ...,  0.0431,  0.0039,  0.0039],\n",
       "           [ 0.4039,  0.4039,  0.4275,  ...,  0.0431,  0.0118, -0.0118]],\n",
       " \n",
       "          [[ 0.4118,  0.4275,  0.4510,  ..., -0.1294, -0.1451, -0.1529],\n",
       "           [ 0.3961,  0.4275,  0.4275,  ..., -0.1137, -0.1529, -0.1765],\n",
       "           [ 0.4118,  0.4353,  0.4118,  ..., -0.1137, -0.1216, -0.2000],\n",
       "           ...,\n",
       "           [ 0.2000,  0.1922,  0.1922,  ..., -0.0980, -0.1137, -0.1059],\n",
       "           [ 0.1843,  0.2078,  0.2157,  ..., -0.0824, -0.1294, -0.1216],\n",
       "           [ 0.1608,  0.2235,  0.2235,  ..., -0.0824, -0.1373, -0.1294]],\n",
       " \n",
       "          [[ 0.0353,  0.0353,  0.0902,  ..., -0.4824, -0.4902, -0.4824],\n",
       "           [-0.0118,  0.0196,  0.0431,  ..., -0.4824, -0.5216, -0.5216],\n",
       "           [ 0.0353,  0.0745,  0.0353,  ..., -0.4824, -0.4902, -0.5451],\n",
       "           ...,\n",
       "           [-0.0588, -0.0745, -0.0667,  ..., -0.4118, -0.4353, -0.4353],\n",
       "           [-0.0745, -0.0588, -0.0431,  ..., -0.3961, -0.4353, -0.4353],\n",
       "           [-0.1059, -0.0588, -0.0431,  ..., -0.4039, -0.4353, -0.4431]]]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza os inputs\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d6b40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloca o modelo em modo de avaliação\n",
    "trainer.model.eval() \n",
    "\n",
    "# Desliga os gradientes para a inferência\n",
    "with torch.no_grad():  \n",
    "    outputs = trainer.model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "574a82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os logits\n",
    "logits = outputs.logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bbb2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai o logit de maior valor para a imagem\n",
    "class_index = logits.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11d12f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extrai o valor que desejamos do tensor\n",
    "valor = class_index.item()\n",
    "valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8891882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o mapeamento original (está na documentação do dataset bean)\n",
    "mapping = {\n",
    "  \"angular_leaf_spot\": 0,\n",
    "  \"bean_rust\": 1,\n",
    "  \"healthy\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e4a6a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'angular_leaf_spot', 1: 'bean_rust', 2: 'healthy'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cria um mapeamento reverso\n",
    "reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "reverse_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cac8bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa o mapeamento reverso para obter o nome da classe\n",
    "class_name = reverse_mapping.get(valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3be7583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A imagem foi classificada como: angular_leaf_spot\n"
     ]
    }
   ],
   "source": [
    "print(\"A imagem foi classificada como:\", class_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

![desktop-wallpaper-explained-deep-learning-in-tensorflow-artificial-intelligence](https://user-images.githubusercontent.com/91103250/231323947-143a2057-c83f-4438-8e6a-7171768dd9fd.png)

# Deep Learning Para Aplicações de IA com PyTorch e Lightning
O primeiro curso da formação com Inteligência Artificial onde é construído modelos de Deep Learning com as principais arquiteturas, como realizar o pré-processamento dos dados, como otimizar os modelos e como realizar o deploy. Os frameworks PyTorch e Pytorch Lightning serão usados em Linguagem Python através do Jupyter Notebook.

Deep Learning é a principal técnica de Inteligência Artificial, com o framework de maior sucesso da atualidade, o PyTorch. Além disso, também é utilizado o Lightning, uma biblioteca que simplifica a forma como é criado os modelos com o PyTorch aumentando a produtividade e a performance das aplicações de Inteligência Artificial.

# Projetos do Curso
## Lab01 - Anatomia Rede Neural
Este Jupyter Notebook demonstra um exemplo prático de utilização de análise preditiva com um modelo de redes neurais. O exercício faz parte da formação Inteligência Artificial da Data Science Academy. O modelo treinado passa por todas as etapas principais para preparação, treino e avaliação do modelo de rede neural. Neste exerício foi usando o Pytorch e o Ligthning com o uso de linguagem Python.


## Lab02 - Análise Preditiva com Reconhecimento de Imagens
Este script demonstra um exemplo prático de utilização de análise preditiva com um modelo de redes neurais para reconhecimento de imagens. O exercício faz parte da formação Inteligência Artificial da Data Science Academy. O desafio do exercício foi a construção do framework para classificar as imagens.

O modelo treinado passa por todas as etapas principais para preparação, treino e avaliação do modelo de machine learning. Neste exerício foi usando a linguagem Python. Este repositório tem como objetivo a demonstração de análise preditiva com Inteligência Artificial para reconhecimento de imagens.

Fonte dos dados: https://www.cs.toronto.edu/~kriz/cifar.html

## Lab03 - Reconhecimento de Imagens de Satélite no Torchvision
Neste Jupyter Notebook será utilizado técnicas de reconhecimento de imagens de satélite com Torchvision através da linguagem Python. Outro ponto desta análise é verificar o impacto e interpretação de diferentes funções de ativação na construção de um modelo de alta precisão.

Será usado o Torchvision para construir um modelo de Visão Computacional com arquitetura de Deep Learning para classificar as imagens, avaliando o impacto e interpretação no uso de funções de ativação. Ao final teremos um modelo de classificação de imagens de satélites.

Fonte dos dados: https://www.kaggle.com/datasets/mahmoudreda55/satellite-image-classification

## Estudo de Caso 1 - Funções de Ativação
Em muitos problemas de Ciência de Dados, nem sem sempre a entrada X tem uma relação linear com a saída Y. Por exemplo, o preço de um imóvel tem n variáveis que podem influenciar para mais ou menos o preço de uma casa. Com uma análise padrão, nem sempre conseguimos achar alguma resposta clara de como é a relação em X com Y e então usamos as chamadas funções de ativação.

As funções de ativação são usadas em modelos com Deep Learning para introduzir a não linearidade na análise preditiva. Ou seja, em muitos problemas temos uma relação não linear entre as variáveis preditoras e a variável target.

Algumas das funções de ativação mais comuns incluem a ReLU (unidade linear retificada), a sigmóide e a tangente hiperbólica. O objetivo desse script é demonstrar na prática quais as principais diferentes entre elas na performance do modelo. Aa funções escolhidas foram ReLU, LeakyReLU, Sigmoid e Tanh.

O objetivo deste script é demonstrar via Python as diferenças entre funções de ativação para análises com Deep Learning.

## Estudo de Caso 2 - Otimização e Inicialização com Redes Neurais
Este Jupyter Notebook tem como objetivo a demonstração prática dos efeitos dos diferentes processos de inicialização e otimização para modelos com redes neurais.

O processo de otimização em Deep Learning é o processo de encontrar os melhores pesos para a rede neural, de modo que ela possa realizar bem em tarefas específicas, como classificação ou regressão. Alguns exemplos são o SGD, Momentum e Adagrad.

O processo de inicialização em Deep Learning é o processo de definir os valores iniciais dos pesos das camadas da rede neural antes do treinamento começar. A escolha adequada dos valores iniciais é importante para garantir que a rede converge para uma solução ótima durante o treinamento. Existem várias técnicas diferentes de inicialização, como inicialização aleatória, inicialização com zero, inicialização com valores pequenos próximos a zero, entre outras.
